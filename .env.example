# --- Primary LLM Provider Selection ---
# Define which provider configuration block below should be used.
# Options: "openai", "azure_openai", "aws_bedrock", "cerebras", "ollama"
# If unset, invalid, or the chosen provider fails, it will attempt to fallback to "openai".
PRIMARY_LLM_PROVIDER="openai" # <-- SET YOUR DESIRED PROVIDER HERE (e.g., "openai", "azure_openai")

# =========================
# === LLM CONFIGURATIONS ===
# =========================
# Provide configuration for your PRIMARY_LLM_PROVIDER and optionally for the 'openai' fallback.

# --- OpenAI Configuration --- Required for Fallback or if PRIMARY_LLM_PROVIDER="openai" ---
OPENAI_API_KEY="your_openai_api_key" # Required
OPENAI_MODEL_NAME="gpt-4o" # Required (e.g., gpt-4o, gpt-4-turbo, gpt-3.5-turbo)
# OPENAI_API_BASE="" # Optional: For proxies or non-standard endpoints (e.g., LiteLLM)
# OPENAI_TEMPERATURE=0.7 # Optional temperature override

# --- Azure OpenAI Configuration --- Used if PRIMARY_LLM_PROVIDER="azure_openai" ---
# AZURE_OPENAI_API_KEY="your_azure_openai_api_key" # Required
# AZURE_OPENAI_API_BASE="https://your-resource-name.openai.azure.com/" # Required: Endpoint URL
# AZURE_OPENAI_API_VERSION="2024-02-01" # Required: Deployment API version
# AZURE_OPENAI_DEPLOYMENT_NAME="your_deployment_name" # Required: Name of your deployment in Azure AI Studio
# AZURE_OPENAI_MODEL_NAME="gpt-4o" # Optional but recommended: Underlying model name for reference
# AZURE_OPENAI_TEMPERATURE=0.7 # Optional

# --- AWS Bedrock Configuration --- Used if PRIMARY_LLM_PROVIDER="aws_bedrock" ---
# Credentials typically picked up via environment (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN) or an IAM role.
# Ensure the execution environment has permissions for bedrock:InvokeModel.
# AWS_REGION_NAME="us-east-1" # Required: Specify the AWS region for Bedrock (e.g., us-east-1, eu-west-1)
# AWS_BEDROCK_MODEL_ID="anthropic.claude-3-sonnet-20240229-v1:0" # Required: The exact Model ID from Bedrock console
# AWS_BEDROCK_TEMPERATURE=0.7 # Optional (often passed via model_kwargs, included here for consistency)

# --- Cerebras Configuration --- Used if PRIMARY_LLM_PROVIDER="cerebras" ---
# CEREBRAS_API_KEY="your_cerebras_api_key" # Required
# CEREBRAS_MODEL_NAME="llama-3.3-70b" # Required: Specify the model name
# CEREBRAS_API_BASE="https://api.cerebras.ai/v1" # Optional: Default Cerebras endpoint
# CEREBRAS_TEMPERATURE=0.7 # Optional

# --- Ollama Configuration --- Used if PRIMARY_LLM_PROVIDER="ollama" ---
# OLLAMA_MODEL="llama3" # Required: Specify the local model name (e.g., llama3, mistral)
# OLLAMA_BASE_URL="http://localhost:11434/v1" # Optional: Default endpoint if running Ollama locally
# OLLAMA_TEMPERATURE=0.7 # Optional

# =========================
# === TOOL API KEYS ETC ===
# =========================

# VirusTotal API Key (Required for ThreatIntelAgent & MalwareAnalysisTool)
VIRUSTOTAL_API_KEY="your_virustotal_api_key"

# Shodan API Key (Optional, required for ExposureAnalystAgent's Shodan tool)
# SHODAN_API_KEY="your_shodan_api_key"

# Hybrid Analysis API Key (Required for MalwareAnalysisTool)
HYBRID_ANALYSIS_API_KEY="your_hybrid_analysis_key"

# GitHub Token (Optional, potentially used by future tools)
# GITHUB_TOKEN="your_github_personal_access_token"

# WAF Analysis Tool API Keys (Required only if using relevant WAF tools)
# IMPERVA_API_KEY="your_imperva_api_key"
# CLOUDFLARE_API_KEY="your_cloudflare_api_key"
# AWS_ACCESS_KEY_ID="your_aws_access_key_id_for_waf" # Note: Separate from potential Bedrock creds
# AWS_SECRET_ACCESS_KEY="your_aws_secret_access_key_for_waf"
# AZURE_CLIENT_ID="your_azure_client_id_for_waf"
# AZURE_CLIENT_SECRET="your_azure_client_secret_for_waf"
# AZURE_TENANT_ID="your_azure_tenant_id_for_waf"

# News API Key (Potentially used by ThreatIntelAgent)
# NEWS_API_KEY="your_news_api_key"

# =========================
# === OTHER SETTINGS ===
# =========================

# API Server Configuration (If running the FastAPI server)
API_HOST=0.0.0.0
API_PORT=8000
API_DEBUG=True

# Security (For API Server)
# CORS_ORIGINS="http://localhost:3000,http://localhost:8000"
# Allowed CORS Origins (Comma-separated)
# Example: ALLOWED_ORIGINS="http://localhost:3000,https://yourfrontend.com"
ALLOWED_ORIGINS=""

# Logging Level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# OpenTelemetry Endpoint (Optional, for exporting traces/metrics)
# OTEL_EXPORTER_OTLP_ENDPOINT="http://localhost:4317"

# Max API Requests Per Minute (Example, adjust as needed)
# MAX_API_REQUESTS_PER_MINUTE=60
